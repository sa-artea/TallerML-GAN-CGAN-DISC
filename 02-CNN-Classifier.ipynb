{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDES NEURONALES PROFUNDAS\n",
    "Utilice el conjunto de datos Fashion-MNIST para construir un clasificador convolucional de imágenes de productos. Para la construcción del modelo utilice los dos esquemas que se describen a continuación y compare los resultados:\n",
    "\n",
    "1. Entrenar un Clasificador Convolucional de imágenes de productos basado en el Autocodificador convolucional de la práctica 1.\n",
    "2. Clasificar las imágenes de productos utilizando un modelo de clasificación convolucional profundo.\n",
    "3. Evalué el desempeño del clasificador en el conjunto de prueba y muestre la matriz de confusión.\n",
    "\n",
    "Compruebe con ejemplos que el modelo es capaz de clasificar correctamente las imágenes de productos.\n",
    "\n",
    "## OBJETIVO\n",
    "Aplicar el proceso de aprendizaje a partir de datos para resolver problemas de clasificación utilizando redes neuronales convolucionales profundas sobre la herramienta Keras.\n",
    "\n",
    "## DATOS\n",
    "Incluidos en Keras.\n",
    "También, existe otra fuente equivalente que se consigue en el siguiente URL https://www.kaggle.com/zalando-research/fashionmnist donde hay un resumen de estos datos en el archivo CVS y XLSX.\n",
    "\n",
    "la clasificación para el aprendizaje supervisado es:\n",
    "\n",
    "    Label \tClass\n",
    "    0 \t \tT-shirt/top\n",
    "    1 \t \tTrouser\n",
    "    2 \t \tPullover\n",
    "    3 \t \tDress\n",
    "    4 \t \tCoat\n",
    "    5 \t \tSandal\n",
    "    6 \t \tShirt\n",
    "    7 \t \tSneaker\n",
    "    8 \t \tBag\n",
    "    9 \t \tAnkle boot\n",
    "\n",
    "**Importante: Lea los comentarios y apuntes del Notebook para tener claridad de los pasos.**\n",
    "### Consideraciones\n",
    "- Utilice sólo los conjuntos de datos indicado.\n",
    "- El frameworks a utilizar es TensorFlow, Keras con Jupyter Notebbooks.\n",
    "\n",
    "### Enlaces de interés\n",
    "- documentación Keras, URL: https://keras.io/models/sequential/\n",
    "- documentación TensorFlow, URL: https://www.tensorflow.org/versions\n",
    "- Tutorial CNN basico, URL: https://www.kaggle.com/nhlr21/deep-keras-cnn-tutorial/notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando dependencias de trabajo\n",
    "# importando librerias basicas\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# importando modulos de analisis de datos, ML y graficas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from collections import Counter\n",
    "\n",
    "# importando dependencias para tensorflow\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# importando para sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# importando para keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones Utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que recibe una lista numpy y recupera la forma de cada elemento, devuelve una lista con formas\n",
    "def get_shape(data):\n",
    "    # respuesta de la funcion\n",
    "    ans = list()\n",
    "\n",
    "    for d in data:\n",
    "        sp = d.shape\n",
    "        ans.append(sp)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que transforma el entero de la clase a la palabra de la etiqueta, devuelve una lista de etiquetas\n",
    "def class2label(data, labels):\n",
    "    # respuesta de la funcion\n",
    "    ans = list()\n",
    "\n",
    "    for d in data:\n",
    "        d = int(d)\n",
    "        l = str(labels[d])\n",
    "        ans.append(l)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que estandariza los datos en numpy de acuerdo a un valor min & max, devuelve un arreglo np flotante\n",
    "def std_data(data, minv, maxv):\n",
    "    rangev = maxv - minv\n",
    "    ans = data.astype(\"float32\")/float(rangev)\n",
    "    # ans = pd.Series(ans)\n",
    "    # respuesta de la funcion\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar y Preparar los Datos\n",
    "\n",
    "Los pasos de esta seccion son:\n",
    "\n",
    "1. Leer los datos desde MNIST.\n",
    "2. Formatear los datos para que los acepte el DataFrame de Pandas.\n",
    "2. Crear el DataFrame de Pandas con un esquema propio.\n",
    "2. Formatear los datos MNIST para pobrar el DataFrame de pandas.\n",
    "3. Revisar que todo este como deberia estar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de nombres de las clasificaciones\n",
    "label_names = [     # label number\n",
    "    \"T-shirt/top\",      # 0\n",
    "    \"Trouser\",          # 1\n",
    "    \"Pullover\",         # 2\n",
    "    \"Dress\",            # 3\n",
    "    \"Coat\",             # 4\n",
    "    \"Sandal\",           # 5\n",
    "    \"Shirt\",            # 6\n",
    "    \"Sneaker\",          # 7\n",
    "    \"Bag\",              # 8\n",
    "    \"Ankle boot\"        # 9\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se carga el archivo de datos de trabajo por medio de Keras\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombres de columnas para el dataframe de pandas\n",
    "col_names = [           # nombre de columna DF\n",
    "    \"img_data\",         # datos de la imagen\n",
    "    \"img_shape\",        # forma de la imagen\n",
    "    \"class\",            # clase de la imagen, de 0 a 9\n",
    "    \"label\",            # nombre de la clase\n",
    "    \"std_img_data\",     # datos de la imagen estandarizados\n",
    "    \"cat_labels\"        # etiquetas categoricas de la clase\n",
    "]\n",
    "# \"ReshapeData\", \"Label\", \"Class\", \"DataSize\", \"ReshapeSize\", \"ResKeras\", \"ScoreKeras\"]\n",
    "# creando dataframe con columnas\n",
    "fashion_df = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrando datos de mnist\n",
    "img_data = np.concatenate((x_train, x_test), axis=0)\n",
    "class_data = np.concatenate((y_train, y_test), axis=0)\n",
    "# recuperando forma de imagenes\n",
    "img_shape = get_shape(img_data)\n",
    "# recuperando etiquetas de las clases\n",
    "labels = class2label(class_data, label_names)\n",
    "# estandarizar los datos de la imagen\n",
    "std_img_data = std_data(img_data, 0, 255)\n",
    "# categorizando las clases a aprender\n",
    "cat_labels = to_categorical(class_data, len(label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambio de formato para utilizar el dataframe\n",
    "img_data = img_data.tolist()\n",
    "std_img_data = std_img_data.tolist()\n",
    "cat_labels = cat_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir arreglo basico de datos\n",
    "data_lt = (\n",
    "    img_data,       # datos de la imagen   0\n",
    "    img_shape,      # forma de la imagen   1\n",
    "    class_data,     # clase de la imagen   2\n",
    "    labels,         # nombre de la clase   3\n",
    "    std_img_data,   # datos de la imagen estandarizados 4\n",
    "    cat_labels,     # etiquetas categoricas de la clase 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poblamdo las columnas del dataframe\n",
    "for col, data in zip(col_names, data_lt):\n",
    "    fashion_df[col] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprime el encabezado del dataframe\n",
    "fashion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libero memoria\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisando los Datos\n",
    "Es importante revisar los datos para saber que se esta trabajando con ellos. En esta sección se revisa la forma de los datos, se visualizan algunas imágenes y se revisa la distribución de las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruebo imagenes del dataset\n",
    "# imagen de 5x5 subplots con tamaño de 10x10\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.figure(figsize=(10, 10))\n",
    "# recorriendo las imagenes\n",
    "for i in range(25):\n",
    "    # creando subplots\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    # xticks y yticks desactivados\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    # tomo la imagen de la columna de datos de la imagen\n",
    "    # el mapa de colores es viridis de matplotlib\n",
    "    # otras opciones son: \"plasma\", \"viridis\", \"BuPu\", \"hsv\", \"Spectral\"\n",
    "    plt.imshow(fashion_df[\"img_data\"][i],\n",
    "               cmap=plt.cm.Spectral)\n",
    "    # tomo el nombre de la clase de la columna de etiquetas\n",
    "    plt.xlabel(fashion_df[\"label\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesar los Datos\n",
    "\n",
    "Los pasos de esta seccion son:\n",
    "\n",
    "1. Revisar que los datos esten bien.\n",
    "2. Elegir la caracteristicas o propiedades de aprendizaje.\n",
    "3. Elegir la variable objetivo del aprendizaje.\n",
    "4. Dividir la conjunto de datos entre las poblaciones de entrenamiento y pruebas.\n",
    "5. Formatear los datos de aprendizaje y objetivo acorde a la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cchequeo la distribucion de datos\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.histplot(fashion_df[col_names[3]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccionando caracteristicas de aprendizaje y variables objetivo\n",
    "# recuperando la forma de las imagenes basado en el primer elemento de la lista\n",
    "\n",
    "# recuperando los valores y ajustando el tensor para la CNN\n",
    "A = fashion_df[col_names[4]]\n",
    "# recuperando los valores de la cateogoria\n",
    "b = fashion_df[col_names[5]].values\n",
    "\n",
    "# fortateo de datos numpy\n",
    "X = np.array([np.array(i, dtype=\"object\") for i in A], dtype=\"object\")\n",
    "y = np.array([np.array(j, dtype=\"object\") for j in b], dtype=\"object\")\n",
    "\n",
    "print(X.shape)\n",
    "# forma basica general de las imagenes\n",
    "imgsh = X[0].shape\n",
    "# ajuste de forma para el modelo CNN\n",
    "X = X.reshape(fashion_df.shape[0], imgsh[0], imgsh[1], 1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semilla para el random\n",
    "rseed = 42\n",
    "\n",
    "# en tamanho de la muestra para pruebas esta entre 0.2 y 0.3\n",
    "train_pop = 0.8\n",
    "test_pop = 1.0 - train_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribuir los datos entre entrenamiento vs. pruebas\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = test_pop, random_state = rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formateo para keras y tensorflow\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=\"float64\")\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=\"float64\")\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=\"float64\")\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=\"float64\")\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir Modelo CNN\n",
    "\n",
    "Los pasos de esta seccion son:\n",
    "\n",
    "1. Definir las variables topologicas de la red neuronal.\n",
    "2. Definir los parametros de optimizacion y aprendizaje del modelo.\n",
    "3. Definir la topologia (capas) del modelo.\n",
    "4. Definir las condiciones de entrenamiento para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defino parametros necesarios para el modelo Autoencoder\n",
    "# parametros para las capas\n",
    "filters = 32    # 32, 64, 128\n",
    "# TODO cambiar el numero de neuronas de la capa intermedia\n",
    "# mid_neurons, v1 = 7*7, v2 = 28*28\n",
    "mid_neurons = 7*7\n",
    "\n",
    "# TODO cambiar la dimension del reshape convolucional\n",
    "# mid_reshape, v1 = (7, 7, 1), v2 = (28, 28, 1)\n",
    "mid_reshape = (7, 7, 1)\n",
    "ksize = (3, 3)\n",
    "\n",
    "# TODO cambiar el tamano del pool para reducir la imagen\n",
    "# psize, v1 = (2, 2), v2 = (1, 1)\n",
    "psize = (2, 2)\n",
    "\n",
    "# numero de categorias en la salida\n",
    "categories = len(label_names)\n",
    "# numero de neuronas en la capa de salida\n",
    "outn = categories\n",
    "\n",
    "# funcion de activacion para las capas convolucionales\n",
    "# otras opciones son: # \"relu\", \"LeakyReLU\"\n",
    "act = \"LeakyReLU\"\n",
    "# funcion de activacion para la capa de salida\n",
    "# otras opciones son: # \"sigmoid\", \"softmax\"\n",
    "out = \"sigmoid\"\n",
    "# padding para las capas convolucionales\n",
    "pad = \"same\"\n",
    "# porcentaje de dropout para imagenes\n",
    "ldrop = 0.2\n",
    "\n",
    "# forma del kernel de entrada y de las capas intermedias\n",
    "inshape = X[0].shape\n",
    "\n",
    "# parametros de optimizacion del modelo\n",
    "# funcion de perdida\n",
    "# otras opciones son: \"categorical_crossentropy\"\n",
    "l = \"categorical_crossentropy\"\n",
    "# metodo de optimizacion\n",
    "opti = \"adam\"\n",
    "# metrica de evaluacion\n",
    "met = [\"accuracy\"]\n",
    "\n",
    "# parametros de la bitacora de entrenamiento del modelo, 1 es verbose\n",
    "ver = 1\n",
    "# numero de epocas\n",
    "epo = 50\n",
    "# tamano del lote de entrenamiento, se recomienda potencia de 2\n",
    "# otras opciones son: 32, 64, 128, 256\n",
    "bs = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definicion de las capas para el Autoencoder\n",
    "layer_lt = (\n",
    "    # capa de entrada\n",
    "    Input(shape=inshape,\n",
    "          name=\"LayIn\"),\n",
    "\n",
    "    # capa convolucional intermedia con regularizacion\n",
    "    Conv2D(filters,\n",
    "           ksize,\n",
    "           activation=act,\n",
    "           padding=pad,\n",
    "           name=\"EnConv1\"),\n",
    "    MaxPooling2D(psize,\n",
    "                 padding=pad,\n",
    "                 name=\"EnPool1\"),\n",
    "    Dropout(ldrop,\n",
    "            name=\"EnDrop1\"),\n",
    "\n",
    "    # capa convolucional intermedia con regularizacion\n",
    "    Conv2D(int(filters/2),\n",
    "           ksize,\n",
    "           activation=act,\n",
    "           padding=pad,\n",
    "           name=\"EnConv2\"),\n",
    "    MaxPooling2D(psize,\n",
    "                 padding=pad,\n",
    "                 name=\"EnPool2\"),\n",
    "    Dropout(ldrop,\n",
    "            name=\"EnDrop2\"),\n",
    "\n",
    "    # capa intermedia de 2D a 1D\n",
    "    Flatten(name=\"LayFlat\"),\n",
    "    # capa intermedia densamente poblada con regularizacion\n",
    "    Dense(mid_neurons,\n",
    "          activation=act,\n",
    "          name=\"DenseMid\"),\n",
    "    Dropout(ldrop,\n",
    "            name=\"MidDrop\"),\n",
    "    \n",
    "    # capa densa para clasificar los datos\n",
    "    Dense(int(mid_neurons)//2,\n",
    "          activation=act,\n",
    "          name=\"ClsDense1\"),\n",
    "    Dropout(ldrop,\n",
    "            name=\"ClsDrop1\"),\n",
    "\n",
    "    # capa densa para clasificar los datos\n",
    "    Dense(int(mid_neurons)//4,\n",
    "          activation=act,\n",
    "          name=\"ClsDense2\"),\n",
    "    Dropout(ldrop,\n",
    "            name=\"ClsDrop2\"),\n",
    "\n",
    "    # capa de salida\n",
    "    Dense(outn,\n",
    "          activation=out,\n",
    "          name=\"LayOut\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiendo el modelo CNN en Keras\n",
    "cnn_model = Sequential(layer_lt)\n",
    "cnn_model.model_name=\"DCNN_Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilando las condiciones de optimizacion y ajuste del Modelo CNN\n",
    "cnn_model.compile(loss=l,\n",
    "                  optimizer=opti,\n",
    "                  metrics=met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resumen de la topologia del modelo\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimo imagen de la topologia del modelo\n",
    "wdir = os.getcwd()\n",
    "folder_models = \"Models\"\n",
    "model_fn = \"discws_cnn_classifier.png\"\n",
    "model_fp = os.path.join(folder_models, model_fn)\n",
    "print(\"La imagen del modelo esta en:\", model_fp)\n",
    "keras.utils.plot_model(cnn_model,\n",
    "                       model_fp,\n",
    "                       show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condiciones de parada temprana\n",
    "cnn_earlystop_acc = EarlyStopping(monitor=\"val_accuracy\",\n",
    "                                  min_delta=0.001,\n",
    "                                  patience=5,\n",
    "                                  verbose=ver,\n",
    "                                  mode=\"max\",\n",
    "                                  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar Modelo\n",
    "\n",
    "Los pasos de esta seccion son:\n",
    "\n",
    "1. Entrenar el modelo con el conjunto de entrenamineto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajustando el modelo MLP Keras\n",
    "cnn_log = cnn_model.fit(\n",
    "    x=X_train,    # np.array(X_trainB),\n",
    "    y=y_train,    # to_categorical(np.array(y_trainB), categories),\n",
    "    batch_size=bs,\n",
    "    epochs=epo,\n",
    "    verbose=ver,\n",
    "    callbacks=[cnn_earlystop_acc],\n",
    "    workers=8,\n",
    "    shuffle=False,\n",
    "    use_multiprocessing=True,\n",
    "    validation_data=(X_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probar Modelo\n",
    "\n",
    "Los pasos de esta seccion son:\n",
    "\n",
    "1. Probar el modelo con el conjunto de pruebas.\n",
    "2. Evaluar globalmente los resultados.\n",
    "3. Guardar el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_eval = cnn_model.evaluate(x=X_test,\n",
    "                              y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultados generales\n",
    "print(\"Perdida promedio: \", cnn_eval[0])\n",
    "print(\"Precision promedio: \", cnn_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruebas sobre el modelo\n",
    "cnn_predictions = cnn_model.predict(X_test,\n",
    "                                    verbose=ver)\n",
    "print(cnn_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar el modelo entrenado\n",
    "wdir = os.getcwd()\n",
    "folder_models = \"Models\"\n",
    "model_fn = \"discws_cnn_classifier.h5\"\n",
    "model_fp = os.path.join(folder_models, model_fn)\n",
    "print(\"El modelo entrenado esta en:\", model_fp)\n",
    "cnn_model.save(model_fp)\n",
    "# tf.keras.models.save_model(cnn_autoencoder, model_fpn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrar Resultados\n",
    "\n",
    "Los pasos de esta sección son:\n",
    "\n",
    "1. Mostrar las curvas de aprendizaje.\n",
    "2. Mostrar la Matriz de confusión del Clasificador.\n",
    "3. Mostrar la clasificación del modelo.\n",
    "4. Mostrar la abstracción del Clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajuste de las predicciones para ver el reporte de matrix de confusion\n",
    "cnn_count = np.array(cnn_predictions).argmax(axis=1)\n",
    "y_count = np.array(y_test).argmax(axis=1)\n",
    "print(cnn_count.shape)\n",
    "print(y_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparando informe de la matriz de confusion\n",
    "cnn_counter = Counter(cnn_count)\n",
    "cnn_matrix = confusion_matrix(y_count, cnn_count)\n",
    "cnn_report = classification_report(y_count, cnn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informe por consola de las pruebas para el clasificador\n",
    "print(\"----- Reporte de Pruebas para el clasificador CNN -----\")\n",
    "print(\"--- Conteo ---\\n\", cnn_counter)\n",
    "print(\"--- Matriz de Confusion ---\\n\", cnn_matrix)\n",
    "print(\"--- Reporte de Pruebas: ---\")\n",
    "print(cnn_report)\n",
    "print(\"--- Puntaje General---\\n\")\n",
    "print(\" - Perdida: \", cnn_eval[0])\n",
    "print(\" - Precision: \", cnn_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# despliegue grafico de la matriz de confusion\n",
    "# dataframe para graficar\n",
    "confusion_df = pd.DataFrame(data=cnn_matrix,\n",
    "                            columns=label_names,\n",
    "                            index=label_names)\n",
    "\n",
    "# creando el reporte para graficar\n",
    "cnn_report = classification_report(y_count,\n",
    "                                   cnn_count,\n",
    "                                   output_dict=True)\n",
    "# dataframe del repodrte\n",
    "report_df = pd.DataFrame(data=cnn_report)\n",
    "\n",
    "# dataframe para desplegar el puntaje de las clases\n",
    "cols = [\n",
    "    \"0\",\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\"\n",
    "]\n",
    "idx = [\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"f1-score\"\n",
    "]\n",
    "class_report = pd.DataFrame(report_df[cols],\n",
    "                            index=idx)\n",
    "class_report = pd.DataFrame(class_report.values,\n",
    "                            columns=label_names,\n",
    "                            index=idx)\n",
    "class_report = class_report.T\n",
    "\n",
    "# dataframe para desplegar el reporte global\n",
    "cols = [\n",
    "    \"accuracy\",\n",
    "    \"macro avg\",\n",
    "    \"weighted avg\"\n",
    "]\n",
    "macro_report = pd.DataFrame(report_df[cols],\n",
    "                            index=idx)\n",
    "macro_report = macro_report.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapa de calor de la matriz de confusion por conteo\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.set_theme(style=\"white\", font_scale=1.6)\n",
    "sns.heatmap(confusion_df, annot=True, fmt='.1f', cmap=\"BuPu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# presicion, recall y f1-score de las clases en el modelo\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.set_theme(style=\"white\", font_scale=1.6)\n",
    "sns.heatmap(class_report, annot=True, fmt='.2f', cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.set_theme(style=\"white\", font_scale=1.6)\n",
    "sns.heatmap(macro_report, annot=True, fmt='.2f', cmap=\"Greens\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reporte del aprendizaje\n",
    "# base de la figura\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# datos de la figura en de perdida y precision\n",
    "ax1.plot(cnn_log.history[\"loss\"], 'red', label=\"Train Loss\")\n",
    "ax1.plot(cnn_log.history[\"val_loss\"], 'darkorange', label=\"Test Loss\")\n",
    "ax2.plot(cnn_log.history[\"accuracy\"], 'red', label=\"Train Accuracy\")\n",
    "ax2.plot(cnn_log.history[\"val_accuracy\"], 'royalblue', label=\"Test Accuracy\")\n",
    "\n",
    "# leyenda de la grafica\n",
    "fig.suptitle(\"LEARNING BEHAVIOR\")\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "ax1.set_title(\"Loss\")\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax1.set(xlabel=\"Epoch [cycle]\", ylabel=\"loss [%]\")\n",
    "ax2.set(xlabel=\"Epoch [cycle]\", ylabel=\"Acc [%]\")\n",
    "fig.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prueba funcional del autoencoder\n",
    "max_img = 10\n",
    "random_test_img = np.random.randint(len(X_test),\n",
    "                                    size=max_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classprob_df = pd.DataFrame(columns=random_test_img,\n",
    "                            index=label_names)\n",
    "\n",
    "for img in random_test_img:\n",
    "    temp_cls = np.array(cnn_predictions[img])\n",
    "    classprob_df[img] = temp_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_layer = \"LayFlat\"\n",
    "cnn_abstraction = Model(inputs=cnn_model.input,\n",
    "                        outputs=cnn_model.get_layer(middle_layer).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desplegando pruebas\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.figure(figsize=(20, 10))\n",
    "og_shape = fashion_df[col_names[1]][0]\n",
    "og_label = fashion_df[col_names[3]][0]\n",
    "\n",
    "for i, img in enumerate(random_test_img):\n",
    "    # imagen original\n",
    "    ax = plt.subplot(3, max_img, i + 1)\n",
    "    temp_X = np.array(X_test[img])\n",
    "    # imprimir el nombre de la imagen\n",
    "    plt.title(img)\n",
    "    # ajustar ty tamaño y color al original\n",
    "    plt.imshow(temp_X.reshape(og_shape),\n",
    "               cmap=plt.cm.Spectral)\n",
    "    # imprimir el nombre de la clase\n",
    "    idx = list(y_test[img]).index(max(y_test[img]))\n",
    "    # print(idx, label_names[idx])\n",
    "    # print(label_names)\n",
    "    plt.xlabel(label_names[idx])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # imagen abstracta de la capa intermedia del autoencoder\n",
    "    ax = plt.subplot(3, max_img, max_img + i + 1)\n",
    "    # recuperar la imagen abstracta\n",
    "    temp_abstract = cnn_abstraction(temp_X)\n",
    "    # formato de la imagen abstracta\n",
    "    temp_abstract = np.array(temp_abstract)\n",
    "    # ajusta tamaño y color\n",
    "    px = temp_abstract.shape[0]*temp_abstract.shape[1]\n",
    "    px = int(np.sqrt(px))\n",
    "    plt.imshow(temp_abstract.reshape(px, px),\n",
    "               cmap=plt.cm.viridis)\n",
    "    # imprimir nombre de la clase\n",
    "    plt.xlabel(label_names[idx])\n",
    "\n",
    "\n",
    "    # clasificacion aproximada\n",
    "    temp_cls = np.array(cnn_predictions[img])\n",
    "    temp_cls = temp_cls.argmax(axis=0)\n",
    "    temp_cls = int(temp_cls)\n",
    "    temp_cls = label_names[temp_cls]\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "sns.set_theme(style=\"white\", font_scale=1.8)\n",
    "sns.heatmap(classprob_df, annot=True, fmt='.3f', cmap=\"BuPu\")\n",
    "plt.tick_params(axis='both',\n",
    "                which='major',\n",
    "                labelbottom = False,\n",
    "                bottom=False,\n",
    "                top=False,\n",
    "                labeltop=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
